{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP4wpzfM9bMC",
        "outputId": "f125d45c-9190-4bf8-d053-1cee050ea546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.51.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: simpy in /usr/local/lib/python3.12/dist-packages (4.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.4.1)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.10.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.28.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit pandas numpy matplotlib seaborn scikit-learn simpy joblib pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_streamlit.py\n",
        "\n",
        "# --- ARQUIVO: app_streamlit.py ---\n",
        "# (CORREÃ‡ÃƒO: Convertido anomalous_ids para list() antes de passar para a funÃ§Ã£o cacheada)\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import simpy\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from typing import Optional, List, Tuple, Dict\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# =======================================================\n",
        "# SEÃ‡ÃƒO 1 - TEORIA DE FILAS (M/M/c)\n",
        "# =======================================================\n",
        "\n",
        "class MMCQueue:\n",
        "    def __init__(self, arrival_rate, service_rate, servers):\n",
        "        self.lambda_ = arrival_rate\n",
        "        self.mu = service_rate\n",
        "        self.c = servers\n",
        "\n",
        "    def utilization(self):\n",
        "        if self.c == 0 or self.mu == 0:\n",
        "            return float('inf') if self.lambda_ > 0 else 0.0\n",
        "        return self.lambda_ / (self.c * self.mu)\n",
        "\n",
        "    def _p0(self):\n",
        "        rho = self.utilization()\n",
        "        c = self.c\n",
        "        if rho >= 1:\n",
        "            return 0.0\n",
        "        try:\n",
        "            sum_terms = sum((c * rho) ** n / math.factorial(n) for n in range(c))\n",
        "            last_term = ((c * rho) ** c) / (math.factorial(c) * (1 - rho))\n",
        "            return 1 / (sum_terms + last_term)\n",
        "        except (OverflowError, ValueError):\n",
        "            return 0.0\n",
        "\n",
        "    def avg_waiting_time_in_queue(self):\n",
        "        rho = self.utilization()\n",
        "        c = self.c\n",
        "        if rho >= 1:\n",
        "            return float('inf')\n",
        "        p0 = self._p0()\n",
        "        if p0 == 0.0:\n",
        "             return float('inf')\n",
        "        try:\n",
        "            numerator = ((c * rho) ** c) * p0\n",
        "            denominator = math.factorial(c) * (1 - rho) ** 2\n",
        "            Lq = numerator / denominator\n",
        "            return (Lq / self.lambda_) if self.lambda_ > 0 else 0.0\n",
        "        except (OverflowError, ValueError):\n",
        "            return float('inf')\n",
        "\n",
        "    def avg_time_in_system(self):\n",
        "        wq = self.avg_waiting_time_in_queue()\n",
        "        if wq == float('inf') or self.mu == 0:\n",
        "            return float('inf')\n",
        "        return wq + (1 / self.mu)\n",
        "\n",
        "# =======================================================\n",
        "# SEÃ‡ÃƒO 2 - SIMULAÃ‡ÃƒO SIMPY (COM BURSTS)\n",
        "# =======================================================\n",
        "\n",
        "def estimate_arrival_rate(df):\n",
        "    df = df.dropna(subset=['timestamp'])\n",
        "    if len(df) < 2: return 0.0001\n",
        "    ts_min, ts_max = df['timestamp'].min(), df['timestamp'].max()\n",
        "    sec = (ts_max - ts_min).total_seconds()\n",
        "    return len(df) / sec if sec > 0 else 0.0001\n",
        "\n",
        "def estimate_service_time(df, mode='exp'):\n",
        "    values = df['login_duration_sec'].dropna().values\n",
        "    if len(values) == 0: return 1.0\n",
        "    return float(np.median(values)) if mode=='exp' else values\n",
        "\n",
        "def generate_bursts(run_time, n_bursts=2, duration=60, mult=3, seed=42):\n",
        "    random.seed(seed)\n",
        "    bursts = []\n",
        "    if run_time <= duration:\n",
        "        return []\n",
        "    for _ in range(n_bursts):\n",
        "        start = random.uniform(0, run_time - duration)\n",
        "        bursts.append((start, start+duration, mult))\n",
        "    return bursts\n",
        "\n",
        "class AuthSystemSim:\n",
        "    def __init__(self, env, servers, service_src, mode):\n",
        "        self.env = env\n",
        "        self.server = simpy.Resource(env, capacity=servers)\n",
        "        self.service_src = service_src\n",
        "        self.mode = mode\n",
        "        self.wait_times = []\n",
        "        self.total = 0\n",
        "\n",
        "    def service_time(self):\n",
        "        if self.mode == 'exp':\n",
        "            m = float(self.service_src)\n",
        "            return random.expovariate(1/m) if m > 0 else 0\n",
        "        return float(np.random.choice(self.service_src))\n",
        "\n",
        "    def process(self, _):\n",
        "        arrival = self.env.now\n",
        "        with self.server.request() as req:\n",
        "            yield req\n",
        "            self.wait_times.append(self.env.now - arrival)\n",
        "            yield self.env.timeout(self.service_time())\n",
        "            self.total += 1\n",
        "\n",
        "def arrival_process(env, system, Î», runtime, bursts):\n",
        "    Î»_max = Î» * max([m for _,_,m in bursts] + [1])\n",
        "    if Î»_max <= 0:\n",
        "        return\n",
        "    while env.now < runtime:\n",
        "        interarrival = random.expovariate(Î»_max)\n",
        "        yield env.timeout(interarrival)\n",
        "        if env.now >= runtime:\n",
        "            break\n",
        "        mult = 1\n",
        "        for start,end,m in bursts:\n",
        "            if start <= env.now <= end:\n",
        "                mult = m\n",
        "                break\n",
        "        if random.random() <= (Î»*mult)/Î»_max:\n",
        "            env.process(system.process(None))\n",
        "\n",
        "@st.cache_data\n",
        "def run_sim(servers, runtime, bursts_tuple, Î», m_service):\n",
        "    bursts = list(bursts_tuple)\n",
        "    env = simpy.Environment()\n",
        "    s = AuthSystemSim(env, servers, m_service, 'exp')\n",
        "    env.process(arrival_process(env, s, Î», runtime, bursts))\n",
        "    env.run(until=runtime)\n",
        "    return np.mean(s.wait_times or [0]), s.total\n",
        "\n",
        "# =======================================================\n",
        "# SEÃ‡ÃƒO 3 - FUNÃ‡Ã•ES DE ANÃLISE DE DADOS (PERNA 1)\n",
        "# =======================================================\n",
        "\n",
        "@st.cache_data\n",
        "def load_data(path):\n",
        "    df = pd.read_csv(path)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "    df = df.dropna(subset=['timestamp'])\n",
        "    if 'timestamp' in df.columns:\n",
        "        df['hour'] = df['timestamp'].dt.hour\n",
        "        df['weekday'] = df['timestamp'].dt.day_name()\n",
        "    return df\n",
        "\n",
        "def plot_bar_with_labels(df, x, y, title, ylabel='Taxa', ylim_pad=0.01, rotate_x=False):\n",
        "    fig, ax = plt.subplots(figsize=(8,4))\n",
        "    if df.empty or y not in df.columns:\n",
        "        st.warning(f\"NÃ£o hÃ¡ dados para plotar '{title}'\")\n",
        "        ax.set_title(title)\n",
        "        st.pyplot(fig)\n",
        "        return\n",
        "\n",
        "    df = df.sort_values(y, ascending=False)\n",
        "    bars = ax.bar(df[x].astype(str), df[y])\n",
        "    min_y = df[y].min()\n",
        "    max_y = df[y].max()\n",
        "    ax.set_title(title)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_ylim(max(0, min_y - ylim_pad), max_y + ylim_pad)\n",
        "    if rotate_x:\n",
        "        plt.xticks(rotation=30, ha='right')\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, height + (ylim_pad/2 if ylim_pad>0 else 0.005), f\"{height:.3f}\", ha='center', fontsize=9)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "def compute_kpis(df):\n",
        "    kpis = {\n",
        "        'total_logins': len(df),\n",
        "        'success_rate': df['login_success'].mean() if 'login_success' in df.columns else None,\n",
        "        'fail_rate': 1 - df['login_success'].mean() if 'login_success' in df.columns else None,\n",
        "        'unique_users': df['user_id'].nunique() if 'user_id' in df.columns else None,\n",
        "        'unique_ips': df['ip_address'].nunique() if 'ip_address' in df.columns else None,\n",
        "        'avg_login_duration_s': df['login_duration_sec'].mean() if 'login_duration_sec' in df.columns else None\n",
        "    }\n",
        "    return kpis\n",
        "\n",
        "@st.cache_data\n",
        "def detect_anomalies_isolationforest(_df, features, contamination=0.01):\n",
        "    df = _df.copy()\n",
        "    iso = IsolationForest(contamination=contamination, random_state=42)\n",
        "    X = df[features].fillna(0).values\n",
        "    if len(X) == 0:\n",
        "        return df, None\n",
        "    iso.fit(X)\n",
        "    df['anomaly_score'] = iso.decision_function(X)\n",
        "    df['is_anomaly'] = (iso.predict(X) == -1).astype(int)\n",
        "    return df, iso\n",
        "\n",
        "@st.cache_data\n",
        "def find_users_with_many_failures(_df, quantile_threshold=0.95, min_failures_threshold=3):\n",
        "    df = _df.copy()\n",
        "    if 'login_success' not in df.columns or 'user_id' not in df.columns:\n",
        "        return pd.DataFrame(columns=['user_id', 'failure_count']), 0, \"Colunas 'user_id' ou 'login_success' em falta.\"\n",
        "\n",
        "    failures = df[df['login_success'] == 0]\n",
        "    if failures.empty:\n",
        "        return pd.DataFrame(columns=['user_id', 'failure_count']), 0, \"NÃ£o foram registadas falhas no dataset.\"\n",
        "\n",
        "    user_failures = failures.groupby('user_id').size().to_frame(name='failure_count')\n",
        "\n",
        "    quantile_val = user_failures['failure_count'].quantile(quantile_threshold)\n",
        "    final_threshold = max(quantile_val, min_failures_threshold)\n",
        "\n",
        "    anomalous_users = user_failures[user_failures['failure_count'] >= final_threshold].sort_values(by='failure_count', ascending=False)\n",
        "    return anomalous_users, final_threshold, None\n",
        "\n",
        "@st.cache_data\n",
        "def get_anomalous_user_details(_df, anomalous_user_ids_list): # O argumento agora espera uma lista\n",
        "    \"\"\"\n",
        "    Filtra o dataframe principal para obter os detalhes de atividade\n",
        "    dos utilizadores anÃ³malos.\n",
        "    \"\"\"\n",
        "    if 'ip_address' not in _df.columns or 'device_type' not in _df.columns:\n",
        "        return pd.DataFrame(), pd.DataFrame(), \"Faltam 'ip_address' ou 'device_type' no dataset.\"\n",
        "\n",
        "    # Filtra toda a atividade (sucessos e falhas) dos utilizadores suspeitos\n",
        "    user_activity = _df[_df['user_id'].isin(anomalous_user_ids_list)]\n",
        "    if user_activity.empty:\n",
        "        return pd.DataFrame(), pd.DataFrame(), \"Nenhuma atividade encontrada para estes utilizadores.\"\n",
        "\n",
        "    # 1. IPs mais usados por este grupo (Top 10)\n",
        "    top_ips = user_activity['ip_address'].value_counts().head(10).to_frame(name='NÂº de Logins (Total)')\n",
        "\n",
        "    # 2. Dispositivos mais usados por este grupo (Top 10)\n",
        "    top_devices = user_activity['device_type'].value_counts().head(10).to_frame(name='NÂº de Logins (Total)')\n",
        "\n",
        "    return top_ips, top_devices, None # None para \"sem erro\"\n",
        "\n",
        "@st.cache_data\n",
        "def cluster_logins(_df, n_clusters=3):\n",
        "    df = _df.copy()\n",
        "    features = [c for c in ['login_duration_sec', 'latency_ms'] if c in df.columns]\n",
        "    if not features:\n",
        "        return df, None, \"Sem features (duraÃ§Ã£o/latÃªncia) para clusterizar.\"\n",
        "\n",
        "    data_to_cluster = df[features].fillna(df[features].median())\n",
        "    if len(data_to_cluster) < n_clusters:\n",
        "        return df, None, \"Menos amostras do que clusters.\"\n",
        "\n",
        "    X = data_to_cluster.values\n",
        "    scaler = StandardScaler()\n",
        "    Xs = scaler.fit_transform(X)\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(Xs)\n",
        "    df['cluster'] = labels\n",
        "    return df, kmeans, f\"Clusterizado com base em: {', '.join(features)}\"\n",
        "\n",
        "@st.cache_data\n",
        "def train_predictive_model(_df):\n",
        "    df = _df.copy()\n",
        "    feats = [c for c in ['login_duration_sec', 'latency_ms'] if c in df.columns]\n",
        "\n",
        "    if not feats or 'login_success' not in df.columns:\n",
        "        return None, \"Faltam 'login_success' ou features (duraÃ§Ã£o/latÃªncia).\"\n",
        "\n",
        "    data = df.copy().dropna(subset=feats + ['login_success'])\n",
        "    if len(data) < 10:\n",
        "        return None, \"Poucos dados (apÃ³s dropna) para treinar.\"\n",
        "\n",
        "    X = data[feats].values\n",
        "    y = data['login_success'].astype(int).values\n",
        "\n",
        "    if len(np.unique(y)) < 2:\n",
        "        return None, \"Modelo requer duas classes (sucesso/falha) nos dados.\"\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "    model = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
        "    acc = model.score(X_test, y_test)\n",
        "    return model, {'accuracy': acc, 'features': feats}\n",
        "\n",
        "\n",
        "# =======================================================\n",
        "# SEÃ‡ÃƒO 4 - INTERFACE STREAMLIT (UI COMPLETA)\n",
        "# =======================================================\n",
        "\n",
        "st.set_page_config(layout=\"wide\")\n",
        "st.title(\"ðŸ” Dashboard Completo: AnÃ¡lise e SimulaÃ§Ã£o de Login 2FA\")\n",
        "\n",
        "# --- BARRA LATERAL (SIDEBAR) ---\n",
        "with st.sidebar:\n",
        "    st.header(\"1. Carga de Dados\")\n",
        "    path = st.text_input(\"Caminho do CSV:\", \"/content/simulacao_login_dataset1_processed.csv\")\n",
        "\n",
        "    st.header(\"2. Perna 1: AnÃ¡lise de Dados\")\n",
        "    contamination = st.slider(\"ContaminaÃ§Ã£o (Anomalias NumÃ©ricas)\", 0.001, 0.1, 0.01, step=0.001, format=\"%.3f\")\n",
        "    n_clusters = st.slider(\"NÃºmero de clusters (KMeans)\", 2, 8, 3)\n",
        "\n",
        "    min_failures = st.number_input(\"Anomalia: MÃ­n. Falhas/Utilizador\", min_value=1, max_value=50, value=5)\n",
        "\n",
        "    st.header(\"3. Perna 2: Modelagem de Filas\")\n",
        "    st.subheader(\"Teoria M/M/c\")\n",
        "    servers_m = st.slider(\"Servidores (Teoria):\", 1, 20, 3)\n",
        "\n",
        "    st.subheader(\"SimulaÃ§Ã£o SimPy (Estresse)\")\n",
        "    servers_test = st.multiselect(\"Servidores para Simular:\", [1, 2, 3, 4, 5, 6, 7, 8], default=[1, 2, 3, 4])\n",
        "    n_bursts = st.slider(\"NÂº de bursts (picos):\", 0, 10, 3)\n",
        "    mult_bursts = st.slider(\"Multiplicador dos bursts:\", 1.0, 2000.0, 10.0, step=10.0)\n",
        "    dur_bursts = st.slider(\"DuraÃ§Ã£o bursts (s):\", 30, 600, 120, step=30)\n",
        "    runtime = st.number_input(\"DuraÃ§Ã£o total da simulaÃ§Ã£o (s):\", value=3600, min_value=600, step=300)\n",
        "\n",
        "# --- CARGA DOS DADOS ---\n",
        "try:\n",
        "    df = load_data(path)\n",
        "except FileNotFoundError:\n",
        "    st.error(f\"Arquivo nÃ£o encontrado em '{path}'. FaÃ§a o upload do seu CSV para o Colab (no Ã­cone de pasta ðŸ“ Ã  esquerda).\")\n",
        "    st.stop()\n",
        "except Exception as e:\n",
        "    st.error(f\"Erro ao carregar CSV: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "\n",
        "# =======================================================\n",
        "# ABA 1: PERNA 1 - ANÃLISE DE DADOS\n",
        "# =======================================================\n",
        "st.header(\"PERNA 1: AnÃ¡lise de CiÃªncia de Dados (O que aconteceu?)\")\n",
        "\n",
        "# --- KPIs ---\n",
        "st.subheader(\"Resumo do dataset (KPIs)\")\n",
        "kpis = compute_kpis(df)\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "col1.metric(\"Total de logins\", kpis['total_logins'])\n",
        "col2.metric(\"Taxa de sucesso\", f\"{kpis['success_rate']:.2%}\" if kpis['success_rate'] is not None else \"N/A\")\n",
        "col3.metric(\"Utilizadores Ãºnicos\", kpis['unique_users'] if kpis['unique_users'] is not None else \"N/A\")\n",
        "col4.metric(\"Tempo MÃ©dio Login\", f\"{kpis['avg_login_duration_s']:.3f} s\" if kpis['avg_login_duration_s'] is not None else \"N/A\")\n",
        "\n",
        "# --- GrÃ¡ficos de Barras (Taxa de Sucesso) ---\n",
        "st.subheader(\"ðŸ“Š Taxas de Sucesso por Categoria\")\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    if 'device_type' in df.columns and 'login_success' in df.columns:\n",
        "        device_stats = df.groupby('device_type')['login_success'].agg(['mean','count']).reset_index().rename(columns={'mean':'success_rate'})\n",
        "        plot_bar_with_labels(device_stats, 'device_type', 'success_rate', \"Taxa de Sucesso por Dispositivo\")\n",
        "    else:\n",
        "        st.info(\"Faltam colunas 'device_type' ou 'login_success' para este grÃ¡fico.\")\n",
        "with col2:\n",
        "    if 'auth_method' in df.columns and 'login_success' in df.columns:\n",
        "        auth_stats = df.groupby('auth_method')['login_success'].agg(['mean','count']).reset_index().rename(columns={'mean':'success_rate'})\n",
        "        plot_bar_with_labels(auth_stats, 'auth_method', 'success_rate', \"Taxa de Sucesso por MÃ©todo\", rotate_x=True)\n",
        "    else:\n",
        "        st.info(\"Faltam colunas 'auth_method' ou 'login_success' para este grÃ¡fico.\")\n",
        "\n",
        "# --- Histogramas (LatÃªncia e DuraÃ§Ã£o) ---\n",
        "st.subheader(\"â± DistribuiÃ§Ã£o de LatÃªncia e DuraÃ§Ã£o\")\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    if 'latency_ms' in df.columns:\n",
        "        fig, ax = plt.subplots(figsize=(8,4))\n",
        "        df['latency_ms'].plot(kind='hist', bins=50, ax=ax)\n",
        "        ax.set_title(\"DistribuiÃ§Ã£o de LatÃªncia (ms)\")\n",
        "        ax.set_xlabel(\"LatÃªncia (ms)\")\n",
        "        st.pyplot(fig)\n",
        "    else:\n",
        "        st.info(\"Falta a coluna 'latency_ms' para este grÃ¡fico.\")\n",
        "with col2:\n",
        "    if 'login_duration_sec' in df.columns:\n",
        "        fig, ax = plt.subplots(figsize=(8,4))\n",
        "        df['login_duration_sec'].plot(kind='hist', bins=50, ax=ax)\n",
        "        ax.set_title(\"DistribuiÃ§Ã£o de DuraÃ§Ã£o do Login (s)\")\n",
        "        ax.set_xlabel(\"DuraÃ§Ã£o (s)\")\n",
        "        st.pyplot(fig)\n",
        "    else:\n",
        "        st.info(\"Falta a coluna 'login_duration_sec' para este grÃ¡fico.\")\n",
        "\n",
        "# --- AnÃ¡lises de ML (Anomalias, Clustering, PrediÃ§Ã£o) ---\n",
        "st.subheader(\"ðŸ¤– Modelos de ML: Anomalias e Clustering\")\n",
        "col1, col2, col3 = st.columns(3)\n",
        "\n",
        "with col1:\n",
        "    st.markdown(\"**âš ï¸ Anomalias (Comportamento)**\")\n",
        "    af_features = [c for c in ['latency_ms','login_duration_sec'] if c in df.columns]\n",
        "    if af_features:\n",
        "        df_anom, iso = detect_anomalies_isolationforest(df, af_features, contamination=contamination)\n",
        "        n_anom = int(df_anom['is_anomaly'].sum())\n",
        "        st.write(f\"Anomalias detetadas: **{n_anom}** ({contamination*100:.1f}%)\")\n",
        "        if n_anom > 0:\n",
        "            st.dataframe(df_anom[df_anom['is_anomaly']==1][af_features + ['is_anomaly']].head())\n",
        "    else:\n",
        "        st.info(\"Faltam colunas (latÃªncia/duraÃ§Ã£o) para detetar anomalias.\")\n",
        "\n",
        "with col2:\n",
        "    st.markdown(\"**ðŸ”Ž ClusterizaÃ§Ã£o (KMeans)**\")\n",
        "    df_clustered, kmeans_model, cluster_info = cluster_logins(df, n_clusters=n_clusters)\n",
        "    if kmeans_model:\n",
        "        cluster_features = [f for f in ['login_duration_sec','latency_ms'] if f in df_clustered.columns]\n",
        "        st.write(\"Centros dos clusters (medianas):\")\n",
        "        st.dataframe(df_clustered.groupby('cluster')[cluster_features].median().round(3))\n",
        "        st.write(\"Contagem por cluster:\")\n",
        "        st.dataframe(df_clustered['cluster'].value_counts().sort_index().to_frame(name=\"count\"))\n",
        "    else:\n",
        "        st.info(f\"NÃ£o foi possÃ­vel clusterizar: {cluster_info}\")\n",
        "\n",
        "with col3:\n",
        "    st.markdown(\"**ðŸ¤– Modelo Preditivo (Sucesso)**\")\n",
        "    model, model_info = train_predictive_model(df)\n",
        "    if model:\n",
        "        st.write(f\"AcurÃ¡cia do modelo: **{model_info['accuracy']:.2%}**\")\n",
        "        st.write(f\"Features usadas: {', '.join(model_info['features'])}\")\n",
        "    else:\n",
        "        st.info(f\"NÃ£o foi possÃ­vel treinar modelo: {model_info}\")\n",
        "\n",
        "st.divider()\n",
        "\n",
        "# --- SECÃ‡ÃƒO DE UI DE ANÃLISE DE FALHAS (ATUALIZADA) ---\n",
        "st.subheader(\"ðŸ›¡ï¸ AnÃ¡lise de SeguranÃ§a: Utilizadores com MÃºltiplas Falhas (Bots/Ataques)\")\n",
        "\n",
        "anomalous_users_df, threshold, error_msg = find_users_with_many_failures(\n",
        "    df,\n",
        "    min_failures_threshold=min_failures\n",
        ")\n",
        "\n",
        "st.write(f\"A detetar utilizadores com **{min_failures}** ou mais falhas (ou que estejam no top 5% de falhas, o que for maior).\")\n",
        "\n",
        "if error_msg:\n",
        "    st.info(error_msg)\n",
        "elif anomalous_users_df.empty:\n",
        "    st.info(\"Nenhum utilizador encontrado com um nÃºmero anÃ³malo de falhas (acima do limiar).\")\n",
        "else:\n",
        "    st.write(f\"Encontrados **{len(anomalous_users_df)}** utilizadores anÃ³malos (limiar de {int(threshold)} falhas):\")\n",
        "\n",
        "    col1, col2 = st.columns([1, 2])\n",
        "\n",
        "    with col1:\n",
        "        st.write(\"Top 10 Utilizadores Suspeitos:\")\n",
        "        st.dataframe(anomalous_users_df.head(10))\n",
        "\n",
        "    with col2:\n",
        "        st.write(\"GrÃ¡fico: Top 10 Falhas por Utilizador\")\n",
        "        chart_data = anomalous_users_df.head(10).reset_index()\n",
        "        st.bar_chart(chart_data, x='user_id', y='failure_count')\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"#### InvestigaÃ§Ã£o: O que estes utilizadores suspeitos tÃªm em comum?\")\n",
        "\n",
        "    # ObtÃ©m os IDs dos utilizadores suspeitos\n",
        "    anomalous_ids = anomalous_users_df.index\n",
        "\n",
        "    # --- A CORREÃ‡ÃƒO ESTÃ AQUI ---\n",
        "    # Convertemos o objeto 'Index' (que nÃ£o Ã© \"hasheÃ¡vel\") para uma 'list' (que Ã©)\n",
        "    anomalous_ids_list = list(anomalous_ids)\n",
        "\n",
        "    # Chamamos a funÃ§Ã£o cacheada com a lista, e nÃ£o com o Index\n",
        "    top_ips, top_devices, details_error = get_anomalous_user_details(df, anomalous_ids_list)\n",
        "    # --- FIM DA CORREÃ‡ÃƒO ---\n",
        "\n",
        "    if details_error:\n",
        "        st.info(details_error)\n",
        "    else:\n",
        "        col1_details, col2_details = st.columns(2)\n",
        "\n",
        "        with col1_details:\n",
        "            st.write(\"Top IPs Usados por este Grupo:\")\n",
        "            st.dataframe(top_ips)\n",
        "            if not top_ips.empty and top_ips.iloc[0, 0] > len(anomalous_ids):\n",
        "                st.warning(\"ALERTA: Um Ãºnico IP estÃ¡ a ser usado para atacar mÃºltiplas contas!\")\n",
        "\n",
        "        with col2_details:\n",
        "            st.write(\"Top Dispositivos Usados por este Grupo:\")\n",
        "            st.dataframe(top_devices)\n",
        "\n",
        "st.divider()\n",
        "\n",
        "# =======================================================\n",
        "# ABA 2: PERNA 2 - SIMULAÃ‡ÃƒO DE FILAS\n",
        "# =======================================================\n",
        "st.header(\"PERNA 2: SimulaÃ§Ã£o Computacional (O que acontecerÃ¡?)\")\n",
        "\n",
        "# --- PARÃ‚METROS GLOBAIS ---\n",
        "try:\n",
        "    arrival_rate = estimate_arrival_rate(df)\n",
        "    service_time = estimate_service_time(df)\n",
        "    service_rate = 1/service_time\n",
        "except Exception as e:\n",
        "    st.error(f\"Erro ao estimar parÃ¢metros do dataset: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "st.subheader(\"ParÃ¢metros Globais Estimados (Usados nas simulaÃ§Ãµes)\")\n",
        "col1, col2 = st.columns(2)\n",
        "col1.metric(\"Î» (Taxa de Chegada)\", f\"{arrival_rate:.4f} chegadas/s\")\n",
        "col2.metric(\"Î¼ (Taxa de ServiÃ§o)\", f\"{service_rate:.4f} atend./s\", f\"(Tempo mÃ©dio: {service_time:.3f} s)\")\n",
        "\n",
        "st.divider()\n",
        "\n",
        "# --- ANÃLISE TEÃ“RICA ---\n",
        "st.subheader(\"ðŸ“ˆ Teoria M/M/c (CenÃ¡rio de MÃ©dia EstÃ¡vel)\")\n",
        "st.write(f\"AnÃ¡lise teÃ³rica para **{servers_m}** servidores, assumindo fluxo constante e sem picos.\")\n",
        "q = MMCQueue(arrival_rate, service_rate, servers_m)\n",
        "rho = q.utilization()\n",
        "\n",
        "col1, col2, col3 = st.columns(3)\n",
        "col1.metric(f\"Servidores (c)\", servers_m)\n",
        "col2.metric(\"UtilizaÃ§Ã£o (Ï)\", f\"{rho:.3%}\")\n",
        "if rho >= 1.0:\n",
        "    col3.metric(\"Espera MÃ©dia (Wq)\", \"âˆž (InstÃ¡vel)\")\n",
        "    st.warning(f\"Com {servers_m} servidores, o sistema teÃ³rico Ã© instÃ¡vel (Ï â‰¥ 1).\")\n",
        "else:\n",
        "    col3.metric(\"Espera MÃ©dia (Wq)\", f\"{q.avg_waiting_time_in_queue():.4f} s\")\n",
        "\n",
        "st.divider()\n",
        "\n",
        "# --- SIMULAÃ‡ÃƒO PRÃTICA ---\n",
        "st.subheader(\"ðŸ”¬ SimulaÃ§Ã£o com Sobrecarga (Bursts)\")\n",
        "st.write(\"AnÃ¡lise prÃ¡tica que simula picos de trÃ¡fego (bursts) para ver o impacto real no tempo de espera.\")\n",
        "\n",
        "if st.button(\"â–¶ï¸ Executar SimulaÃ§Ã£o de Estresse\", type=\"primary\"):\n",
        "\n",
        "    bursts = generate_bursts(runtime, n_bursts, dur_bursts, mult_bursts)\n",
        "    bursts_tuple = tuple(bursts) # Converte para tupla para o cache\n",
        "\n",
        "    st.write(f\"Simulando {len(servers_test)} cenÃ¡rios com {n_bursts} picos de {mult_bursts}x...\")\n",
        "\n",
        "    with st.spinner(f\"Rodando simulaÃ§Ã£o... Isso pode levar alguns segundos.\"):\n",
        "        results = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for s in servers_test:\n",
        "            w, tot = run_sim(s, runtime, bursts_tuple, arrival_rate, service_time)\n",
        "            results.append([s, w, tot])\n",
        "\n",
        "        end_time = time.time()\n",
        "        st.success(f\"SimulaÃ§Ã£o concluÃ­da em {end_time - start_time:.2f} segundos.\")\n",
        "\n",
        "    res_df = pd.DataFrame(results, columns=[\"Servidores\",\"Espera_MÃ©dia_s\",\"Total_Processado\"])\n",
        "    res_df = res_df.set_index(\"Servidores\")\n",
        "\n",
        "    st.write(\"Resultados da SimulaÃ§Ã£o (com Bursts):\")\n",
        "    st.dataframe(res_df.style.format(\"{:.4f}\"))\n",
        "\n",
        "    st.write(\"Tempo MÃ©dio de Espera vs. Servidores\")\n",
        "    chart_df = res_df.reset_index()\n",
        "\n",
        "    st.line_chart(chart_df, x=\"Servidores\", y=\"Espera_MÃ©dia_s\")\n",
        "\n",
        "st.caption(\"Fim do Dashboard.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrWzJAYB9sky",
        "outputId": "684a8fb9-ab97-4316-efdc-027345a914cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_streamlit.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Encerra tÃºneis ngrok anteriores (se houver) para evitar erros\n",
        "try:\n",
        "    ngrok.kill()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Get the authtoken from Colab secrets\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "if NGROK_AUTH_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "    print(\"ngrok authtoken set.\")\n",
        "else:\n",
        "    print(\"NGROK_AUTH_TOKEN not found in Colab secrets. Please add it.\")\n",
        "\n",
        "\n",
        "# Para o streamlit em segundo plano\n",
        "\n",
        "!streamlit run app_streamlit.py &> /dev/null &\n",
        "\n",
        "# Abre um tÃºnel pÃºblico para a porta 8501 (padrÃ£o do streamlit)\n",
        "# Use connect() without specifying the authtoken again, as it's set globally now\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "\n",
        "print(f\"ðŸŽ‰ Seu app Streamlit COMPLETO estÃ¡ pronto! Acesse aqui: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkn2LVaW93IV",
        "outputId": "1fbcb0e9-3d81-4999-e80f-3d42d8f147f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok authtoken set.\n",
            "ðŸŽ‰ Seu app Streamlit COMPLETO estÃ¡ pronto! Acesse aqui: https://nonalienating-grushie-despina.ngrok-free.dev\n"
          ]
        }
      ]
    }
  ]
}